# ai_server/main.py
import asyncio
from fastapi import FastAPI, HTTPException
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
import uvicorn
import requests
import json

# Ollama API ì—”ë“œí¬ì¸íŠ¸ ì„¤ì •
OLLAMA_API_URL = "http://localhost:11434/api/chat"
OLLAMA_MODEL = "llama3" # ì‚¬ìš©í•  Ollama ëª¨ë¸ ì´ë¦„ (llama3, llama3:70b ë“±)

# FastAPI ì• í”Œë¦¬ì¼€ì´ì…˜ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
app = FastAPI()

# ìš”ì²­ ë°”ë”” ì •ì˜
from typing import List, Dict # List, Dict íƒ€ì… íŒíŠ¸ë¥¼ ìœ„í•´ ì¶”ê°€
class PromptRequest(BaseModel):
    prompt: str
    rag_context: str = "" # RAG ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°›ì„ í•„ë“œ ì¶”ê°€
    few_shot_examples: List[Dict[str, str]] = [] # Few-Shot ì˜ˆì‹œë¥¼ ë°›ì„ ë¦¬ìŠ¤íŠ¸ ë‹¤ì‹œ ì¶”ê°€ (dict í˜•íƒœë¡œ)
    temperature: float = 0.7
    top_p: float = 0.9
    num_predict: int = 512


# LLM ì¶”ë¡  API ì—”ë“œí¬ì¸íŠ¸
@app.post("/generate/")
async def generate_text(request: PromptRequest):
    async def generate_stream():
        try:
            # Ollama APIë¡œ ë³´ë‚¼ ë©”ì‹œì§€ ë°°ì—´ êµ¬ì„±
            ollama_messages = [
                {"role": "system", "content": (
                    "ë‹¹ì‹ ì€ ìˆ˜íŒŒì AI ì±—ë´‡ì…ë‹ˆë‹¤. ëª¨ë“  ì§ˆë¬¸ì—ëŠ” ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”. "
                    "ë‹µë³€í•  ë•Œì—ëŠ” 'ì°¸ê³ ', 'FAQì— ë”°ë¥´ë©´' ë“±ì˜ ì¶œì²˜ í‘œí˜„ì„ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”. "
                    "ì¹œì ˆí•˜ê³  ì •ë¦¬ëœ í˜•íƒœë¡œ ì¤„ë°”ê¿ˆì„ ì ì ˆíˆ í™œìš©í•˜ì—¬ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤. "
                    "ë‹¤ìŒì€ ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ì¶”ê°€ FAQ ì •ë³´ì…ë‹ˆë‹¤ (ì°¸ê³ ìš©):\\n"
                    f"{request.rag_context}" # RAG ì»¨í…ìŠ¤íŠ¸ë¥¼ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ë‚´ì— í¬í•¨
                    "--------------------\\n"
                    "ì´ì „ ëŒ€í™” ì˜ˆì‹œ (ì•„ë˜ Few-Shot ì˜ˆì‹œ)ì™€ ìœ„ FAQ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ ë‹µë³€ì„ ìƒì„±í•˜ì„¸ìš”. ë‹¨, ë‹µë³€ì€ FAQ ë‹µë³€ê³¼ ë§¤ìš° ìœ ì‚¬í•˜ê²Œ ê½¤ë‚˜ ìì„¸í•˜ê³  ì¹œì ˆí•˜ê²Œ ì‘ì„±ë˜ì–´ì•¼ í•©ë‹ˆë‹¤. "
                    "ì œê³µëœ FAQ ì •ë³´ì™€ ì´ì „ ëŒ€í™” ì˜ˆì‹œì—ì„œ ì–»ì€ ì •ë³´ ì™¸ì˜ ë‚´ìš©ì€ ì ˆëŒ€ë¡œ ì–¸ê¸‰í•˜ì§€ ë§ˆì„¸ìš”. "
                    "ë§Œì•½ ì£¼ì–´ì§„ ì •ë³´ë§Œìœ¼ë¡œëŠ” ì‚¬ìš©ì ì§ˆë¬¸ì— ë‹µë³€í•  ìˆ˜ ì—†ë‹¤ë©´, ë‹¹ì‹ ì€ ë°˜ë“œì‹œ "
                    "'ì˜ ëª¨ë¥´ê² ì–´ìš”. ë” ìì„¸í•œ ì •ë³´ê°€ í•„ìš”í•˜ì‹œë©´ ìˆ˜íŒŒì ê³ ê°ì„¼í„°ë¡œ ë¬¸ì˜í•´ì£¼ì„¸ìš”. ğŸ™‡â€â™€ï¸'ë¼ê³ ë§Œ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤. "
                )},
            ]

            # Few-Shot ì˜ˆì‹œ ë©”ì‹œì§€ ì¶”ê°€
            ollama_messages.extend(request.few_shot_examples) 

            # ì‹¤ì œ ì‚¬ìš©ì ì§ˆë¬¸ ì¶”ê°€
            ollama_messages.append({"role": "user", "content": request.prompt}) # request.promptëŠ” ì´ì œ ìˆœìˆ˜í•œ ì‚¬ìš©ì ì§ˆë¬¸

            # Ollama APIë¡œ ë³´ë‚¼ ë°ì´í„° êµ¬ì„± (stream: Trueë¡œ ì„¤ì •)
            data = {
                "model": OLLAMA_MODEL,
                "messages": ollama_messages,
                "stream": True, # ìŠ¤íŠ¸ë¦¬ë° í™œì„±í™”
                "options": {
                    "temperature": request.temperature,
                    "top_p": request.top_p,
                    "num_predict": request.num_predict,
                }
            }

            headers = {
                "Content-Type": "application/json"
            }

            # Ollama API í˜¸ì¶œ (ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µì„ ìœ„í•´ stream=True)
            # requests.postëŠ” ë™ê¸° í•¨ìˆ˜ì´ë¯€ë¡œ, ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ì—ì„œ ì‚¬ìš© ì‹œ ì£¼ì˜ (FastAPIëŠ” ê¸°ë³¸ ë¹„ë™ê¸°)
            # ì§ì ‘ requestsë¥¼ ì‚¬ìš©í•˜ëŠ” ëŒ€ì‹ , `httpx`ì™€ ê°™ì€ ë¹„ë™ê¸° HTTP í´ë¼ì´ì–¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ë” ì¢‹ì§€ë§Œ,
            # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨í•¨ì„ ìœ„í•´ ë™ê¸° requestsë¥¼ ThreadPoolExecutorë¡œ ë˜í•‘í•˜ê±°ë‚˜,
            # FastAPIê°€ ì•Œì•„ì„œ ìŠ¤ë ˆë“œ í’€ì—ì„œ ì‹¤í–‰í•˜ë„ë¡ ë§¡ê¸¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
            # ê·¸ëŸ¬ë‚˜ StreamingResponseì™€ í•¨ê»˜ ì‚¬ìš© ì‹œ `yield`ê°€ ë™ê¸° requestsì˜ blockì„ ê¸°ë‹¤ë¦¬ê²Œ ë˜ì–´ ë¬¸ì œê°€ ë  ìˆ˜ ìˆìœ¼ë¯€ë¡œ,
            # ì‹¤ì œë¡œëŠ” ë¹„ë™ê¸° HTTP í´ë¼ì´ì–¸íŠ¸(httpx)ë¥¼ ì‚¬ìš©í•˜ê±°ë‚˜, generator ë‚´ë¶€ì—ì„œ ë¹„ë™ê¸° ì²˜ë¦¬ë¥¼ í•´ì•¼ í•©ë‹ˆë‹¤.

            # ë¹„ë™ê¸° httpx ì„¤ì¹˜: pip install httpx
            # from httpx import AsyncClient # main.py ìƒë‹¨ì— ì¶”ê°€

            # ì„ì‹œ ë°©í¸ìœ¼ë¡œ, requests.postë¥¼ ë¸”ë¡œí‚¹ í˜¸ì¶œë¡œ ì‚¬ìš©í•˜ë˜, FastAPIì˜ ë°±ê·¸ë¼ìš´ë“œ íƒœìŠ¤í¬ë‚˜
            # yield from await asyncio.to_thread(requests.post, ...) ë“±ì„ ê³ ë ¤í•´ì•¼ í•©ë‹ˆë‹¤.
            # ì—¬ê¸°ì„œëŠ” `requests`ê°€ Blocking I/Oë¥¼ ìˆ˜í–‰í•˜ë¯€ë¡œ, FastAPIì˜ async/await íŒ¨í„´ê³¼ ì¶©ëŒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
            # ê·¸ëŸ¬ë‚˜ í…ŒìŠ¤íŠ¸ ëª©ì ìœ¼ë¡œëŠ” ì‘ë™í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
            # ì‹¤ ì„œë¹„ìŠ¤ì—ì„œëŠ” httpx ê°™ì€ ë¹„ë™ê¸° í´ë¼ì´ì–¸íŠ¸ë¥¼ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.

            with requests.post(OLLAMA_API_URL, headers=headers, json=data, stream=True) as response:
                response.raise_for_status()
                for chunk in response.iter_lines():
                    if chunk:
                        try:
                            parsed_chunk = json.loads(chunk.decode('utf-8'))
                            if "content" in parsed_chunk.get("message", {}):
                                # SSE ë©”ì‹œì§€ ëì— ê³ ìœ í•œ êµ¬ë¶„ì ì¶”ê°€
                                yield f"data: {json.dumps(parsed_chunk['message']['content'])}\nSSE_DELIMITER\n" # <--- ì´ ë¶€ë¶„ ìˆ˜ì •
                            if parsed_chunk.get("done"):
                                # ìŠ¤íŠ¸ë¦¼ ì¢…ë£Œ ì‹œì—ë„ ê³ ìœ í•œ êµ¬ë¶„ì ì‚¬ìš©
                                yield "event: end\ndata: \nSSE_DELIMITER\n" # <--- ì´ ë¶€ë¶„ ìˆ˜ì •
                                break
                        except json.JSONDecodeError:
                            print(f"ê²½ê³ : JSON íŒŒì‹± ì˜¤ë¥˜ - {chunk.decode('utf-8')}")
                            continue
                        await asyncio.sleep(0.01)
        except requests.exceptions.ConnectionError:
            yield f"data: {json.dumps('Ollama ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. Ollamaê°€ ì‹¤í–‰ ì¤‘ì¸ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.')}\\n\\n"
            yield "event: end\\ndata: \\n\\n"
        except requests.exceptions.RequestException as e:
            yield f"data: {json.dumps(f'Ollama API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}')}\\n\\n"
            yield "event: end\\ndata: \\n\\n"
        except Exception as e:
            yield f"data: {json.dumps(f'LLM ì¶”ë¡  ì¤‘ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ ë°œìƒ: {e}')}\\n\\n"
            yield "event: end\\ndata: \\n\\n"

    # Content-Typeì„ "text/event-stream"ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ SSEì„ì„ ì•Œë¦¼
    return StreamingResponse(generate_stream(), media_type="text/event-stream")

# FastAPI ì„œë²„ ì‹¤í–‰
if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8001, reload=True)